{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "828e417c63c782fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scrapping y preprocesamiento de imagenes\n",
    "#### Explicación del Script de Scraping y Procesamiento de Imágenes\n",
    "\n",
    "Este script utiliza la API de Pixabay para descargar imágenes y preparar un dataset organizado para un proyecto de colorización de imágenes usando GANs. El objetivo es recolectar diferentes imágenes por categoría (flores, montañas, animales, objetos cotidianos, entornos urbanos), procesarlas aplicando filtros (eliminar corruptas, de baja resolución, monocromáticas, duplicadas), y guardar 600 imágenes por categoría en formato JPG, redimensionadas a 256x256. \n",
    "\n",
    "Importamos las librerias útiles para el manejo de archivos, procesamiento de imagenes, el calculo del hash, y las solicitudes para recoltar las imagenes con el API de pixabay\n",
    "\n"
   ],
   "id": "739a89705062d0e2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-22T07:01:24.267522Z",
     "start_time": "2025-04-22T07:01:23.695395Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Configuración de las variables del notebook\n",
    "- Estas constantes centralizan la configuración, facilitando ajustes sin modificar el código.\n",
    "- Descargar 1200 imágenes por categoría asegura un margen para descartar imágenes no válidas y alcanzar las 1000 requeridas.\n",
    "- Los directorios TEMP_DIR y OUTPUT_DIR organizan las imágenes en etapas (crudas y procesadas), facilitando la gestión del dataset.\n",
    "- Los umbrales (MIN_SIZE, SATURATION_THRESHOLD, PHASH_THRESHOLD) garantizan que el dataset sea de alta calidad, eliminando imágenes inadecuadas.\n",
    "- REQUESTS_PER_CYCLE y PAUSE_SECONDS respetan el límite de la API de Pixabay (100 solicitudes por 60 segundos), evitando bloqueos."
   ],
   "id": "372a6915d87db2b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:01:24.284050Z",
     "start_time": "2025-04-22T07:01:24.272004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuración\n",
    "PIXABAY_API_KEY = \"49861357-734e1eca2425b5e98b167e58f\"  # Reemplaza con tu clave de API de Pixabay\n",
    "CATEGORIES = [\"flowers\", \"mountains\", \"animals\", \"everyday objects\", \"urban environments\"]\n",
    "IMAGES_PER_CATEGORY = 1200  # Descargar más para tener margen\n",
    "FINAL_IMAGES_PER_CATEGORY = 1000  # Objetivo final\n",
    "TEMP_DIR = \"C:/Users/ctorr/OneDrive/Documentos/Arturo/Arturo HW ITESO/6to semestre/Modelos no lineales/GANs/data/raw\"  # Directorio temporal\n",
    "OUTPUT_DIR = \"C:/Users/ctorr/OneDrive/Documentos/Arturo/Arturo HW ITESO/6to semestre/Modelos no lineales/GANs/data/clean\"  # Directorio final\n",
    "LOG_FILE = \"dataset_preparation.log\"\n",
    "MIN_SIZE = (100, 100)  # Tamaño mínimo\n",
    "TARGET_SIZE = (256, 256)  # Tamaño objetivo\n",
    "SATURATION_THRESHOLD = 0.1  # Umbral de saturación\n",
    "PHASH_THRESHOLD = 5  # Umbral de distancia para pHash\n",
    "REQUESTS_PER_CYCLE = 100  # Máximo de solicitudes por ciclo\n",
    "PAUSE_SECONDS = 60  # Pausa entre ciclos"
   ],
   "id": "90bf5ec87ead7a2c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Configuramos el sistema de logging para registrar eventos (información, advertencias, errores) en el archivo dataset_preparation.log. Esto nos ayuda a rastrear el proceso del script y así poder ver los problemas sin saturar la consola",
   "id": "a706d1da26cc5255"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:01:24.474008Z",
     "start_time": "2025-04-22T07:01:24.459008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configurar logging\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ],
   "id": "46dc3bfe63a767d4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Funciones\n",
    "\n",
    "### Descarga de imagenes\n",
    "- Descargamos imagenes con la api de Pixabay\n",
    "- Pausamos entre los requests para evitar sobrecargar la API\n",
    "- Mostramos la barra de progreso con tqdm\n",
    "- Administramos las descargas de las imagenes en subdirectorios por categoría para hacer más fácil el manejo del dataset\n",
    "\n",
    "### Funciones de filtrado\n",
    "\n",
    "- is_image_corrupt: Verifica si una imagen está corrupta intentando abrirla con PIL. Si falla, la descarta. Las imágenes corruptas podrían causar errores en el entrenamiento de la GAN.\n",
    "- is_low_resolution: Descarta imágenes menores a 100x100 píxeles (MIN_SIZE) para asegurar calidad. Las imágenes de baja resolución no son útiles para un modelo que trabaja con 256x256.\n",
    "- is_monochromatic: Convierte la imagen a HSV y descarta las que tienen saturación promedio menor a 0.1 (SATURATION_THRESHOLD), evitando imágenes en blanco y negro o con colores apagados. Las imágenes monocromáticas son irrelevantes para un proyecto de colorización, ya que carecen de información de color.\n",
    "- get_phash: Calcula un hash perceptual (pHash) para comparar imágenes. El uso de pHash es eficiente para detectar imágenes similares sin comparar píxeles directamente.\n",
    "- remove_duplicates: Compara imágenes usando pHash y elimina las que tienen una distancia menor o igual a 5 (PHASH_THRESHOLD), asegurando que no haya duplicados. Los duplicados reducirían la diversidad del dataset, afectando la generalización del modelo."
   ],
   "id": "f810886ff6028e09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:01:24.505023Z",
     "start_time": "2025-04-22T07:01:24.490008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_images(category, temp_dir, max_images):\n",
    "    \"\"\"Descarga imágenes de Pixabay usando la API.\"\"\"\n",
    "    category_dir = os.path.join(temp_dir, category.replace(\" \", \"_\"))\n",
    "    Path(category_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Mostrar ruta de descarga\n",
    "    print(f\"\\nDescargando imágenes de '{category}' en: {os.path.abspath(category_dir)}\")\n",
    "    logging.info(f\"Iniciando descarga para '{category}' en: {os.path.abspath(category_dir)}\")\n",
    "    \n",
    "    count = 0\n",
    "    page = 1\n",
    "    requests_made = 0\n",
    "    \n",
    "    while count < max_images:\n",
    "        # Construir URL de la API\n",
    "        url = f\"https://pixabay.com/api/?key={PIXABAY_API_KEY}&q={category}&image_type=photo&per_page=100&page={page}\"\n",
    "        \n",
    "        try:\n",
    "            # Realizar solicitud\n",
    "            response = requests.get(url, timeout=10)\n",
    "            requests_made += 1\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                logging.warning(f\"Error en la solicitud para {category}, página {page}: {response.status_code}\")\n",
    "                break\n",
    "            \n",
    "            data = response.json()\n",
    "            hits = data.get('hits', [])\n",
    "            if not hits:\n",
    "                logging.info(f\"No más imágenes para {category} en la página {page}\")\n",
    "                break\n",
    "            \n",
    "            # Descargar imágenes\n",
    "            for i, hit in enumerate(tqdm(hits, desc=f\"Descargando {category}, página {page}\")):\n",
    "                if count >= max_images:\n",
    "                    break\n",
    "                img_url = hit['largeImageURL']\n",
    "                img_path = os.path.join(category_dir, f\"{category.replace(' ', '_')}_{count+1:04d}.jpg\")\n",
    "                try:\n",
    "                    img_response = requests.get(img_url, stream=True, timeout=10)\n",
    "                    if img_response.status_code == 200:\n",
    "                        with open(img_path, 'wb') as f:\n",
    "                            f.write(img_response.content)\n",
    "                        count += 1\n",
    "                    time.sleep(0.2)  # Pausa breve entre descargas\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error al descargar {img_url}: {str(e)}\")\n",
    "            \n",
    "            # Controlar límite de solicitudes\n",
    "            if requests_made >= REQUESTS_PER_CYCLE:\n",
    "                logging.info(f\"Límite de {REQUESTS_PER_CYCLE} solicitudes alcanzado. Pausando {PAUSE_SECONDS} segundos.\")\n",
    "                time.sleep(PAUSE_SECONDS)\n",
    "                requests_made = 0\n",
    "            \n",
    "            page += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error en la solicitud para {category}, página {page}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    logging.info(f\"Descargadas {count} imágenes para {category}\")\n",
    "    return count"
   ],
   "id": "10800defb974942f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:01:24.536529Z",
     "start_time": "2025-04-22T07:01:24.523013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_image_corrupt(file_path):\n",
    "    \"\"\"Verifica si la imagen está corrupta.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(file_path)\n",
    "        img.verify()\n",
    "        img = Image.open(file_path)\n",
    "        img.load()\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Imagen corrupta: {file_path}, Error: {str(e)}\")\n",
    "        return True\n",
    "\n",
    "def is_low_resolution(file_path):\n",
    "    \"\"\"Verifica si la imagen tiene resolución muy baja.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(file_path)\n",
    "        width, height = img.size\n",
    "        if width < MIN_SIZE[0] or height < MIN_SIZE[1]:\n",
    "            logging.info(f\"Imagen de baja resolución descartada: {file_path}, Tamaño: {width}x{height}\")\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "def is_monochromatic(file_path):\n",
    "    \"\"\"Verifica si la imagen es casi monocromática.\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(file_path)\n",
    "        if img is None:\n",
    "            return True\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        saturation = hsv[:, :, 1].mean() / 255.0\n",
    "        if saturation < SATURATION_THRESHOLD:\n",
    "            logging.info(f\"Imagen monocromática descartada: {file_path}, Saturación: {saturation:.3f}\")\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error al verificar monocromía: {file_path}, Error: {str(e)}\")\n",
    "        return True\n",
    "\n",
    "def get_phash(file_path):\n",
    "    \"\"\"Calcula el hash perceptual (pHash).\"\"\"\n",
    "    try:\n",
    "        img = Image.open(file_path)\n",
    "        return imagehash.phash(img)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def remove_duplicates(file_paths):\n",
    "    \"\"\"Elimina imágenes duplicadas basadas en pHash.\"\"\"\n",
    "    hashes = {}\n",
    "    keep_files = []\n",
    "    for file_path in tqdm(file_paths, desc=\"Buscando duplicados\"):\n",
    "        phash = get_phash(file_path)\n",
    "        if phash is None:\n",
    "            logging.warning(f\"No se pudo calcular pHash: {file_path}\")\n",
    "            continue\n",
    "        duplicate = False\n",
    "        for existing_hash in hashes:\n",
    "            if phash - existing_hash <= PHASH_THRESHOLD:\n",
    "                logging.info(f\"Duplicado encontrado: {file_path}, Similar a: {hashes[existing_hash]}\")\n",
    "                duplicate = True\n",
    "                break\n",
    "        if not duplicate:\n",
    "            hashes[phash] = file_path\n",
    "            keep_files.append(file_path)\n",
    "    return keep_files"
   ],
   "id": "31477f0422a49f93",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Funciones de procesamiento\n",
    "\n",
    "#### Process image\n",
    "- La convierte a RGB.\n",
    "- La redimensiona a 256x256 (TARGET_SIZE) usando el método LANCZOS para alta calidad.\n",
    "- La guarda en JPG con calidad 95 en el directorio de salida.\n",
    "- El procesamiento asegura que todas las imágenes en el dataset final tengan un formato consistente (JPG, 256x256, RGB), lo que es necesario para el entrenamiento de la GAN.\n",
    "-Aplicar los filtros en etapas reduce el riesgo de incluir imágenes no válidas en el dataset final.\n",
    "\n",
    "#### Process_category\n",
    "- Crea un subdirectorio en OUTPUT_DIR (e.g., data/clean/flowers/).\n",
    "- Lista todas las imágenes descargadas en TEMP_DIR.\n",
    "- Aplica los filtros en secuencia: elimina corruptas/baja resolución, monocromáticas, y duplicadas.\n",
    "- Procesa hasta max_images (1000) imágenes válidas, guardándolas en el directorio de salida.\n",
    "- Imprime y registra las rutas de origen y destino, así como el número de imágenes procesadas.\n",
    "- Guardar en un directorio separado (data/clean) mantiene las imágenes procesadas organizadas y listas para el entrenamiento.\n",
    "- Las impresiones y logs facilitan el seguimiento del proceso, especialmente para categorías con menos imágenes válidas."
   ],
   "id": "e80a985ac2f759c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:01:24.567514Z",
     "start_time": "2025-04-22T07:01:24.556521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_image(file_path, output_dir, filename):\n",
    "    \"\"\"Procesa una imagen: redimensiona y convierte a JPG.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(file_path).convert('RGB')\n",
    "        img = img.resize(TARGET_SIZE, Image.LANCZOS)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        img.save(output_path, 'JPEG', quality=95)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error al procesar imagen: {file_path}, Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_category(category, temp_dir, output_dir, max_images):\n",
    "    \"\"\"Procesa una categoría completa.\"\"\"\n",
    "    category_temp_dir = os.path.join(temp_dir, category.replace(\" \", \"_\"))\n",
    "    category_output_dir = os.path.join(output_dir, category.replace(\" \", \"_\"))\n",
    "    Path(category_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Mostrar rutas\n",
    "    print(f\"\\nProcesando imágenes de '{category}':\")\n",
    "    print(f\" - Origen: {os.path.abspath(category_temp_dir)}\")\n",
    "    print(f\" - Destino: {os.path.abspath(category_output_dir)}\")\n",
    "    logging.info(f\"Procesando '{category}': Origen: {category_temp_dir}, Destino: {category_output_dir}\")\n",
    "    \n",
    "    # Obtener lista de imágenes descargadas\n",
    "    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "    file_paths = [os.path.join(category_temp_dir, f) for f in os.listdir(category_temp_dir) \n",
    "                  if f.lower().endswith(image_extensions)]\n",
    "    \n",
    "    logging.info(f\"Se encontraron {len(file_paths)} imágenes para {category}\")\n",
    "    \n",
    "    # Filtrar imágenes corruptas y de baja resolución\n",
    "    valid_files = []\n",
    "    for file_path in tqdm(file_paths, desc=f\"Filtrando {category} - Corruptas/Baja resolución\"):\n",
    "        if not (is_image_corrupt(file_path) or is_low_resolution(file_path)):\n",
    "            valid_files.append(file_path)\n",
    "    \n",
    "    # Filtrar imágenes monocromáticas\n",
    "    non_monochromatic_files = []\n",
    "    for file_path in tqdm(valid_files, desc=f\"Filtrando {category} - Monocromáticas\"):\n",
    "        if not is_monochromatic(file_path):\n",
    "            non_monochromatic_files.append(file_path)\n",
    "    \n",
    "    # Eliminar duplicados\n",
    "    unique_files = remove_duplicates(non_monochromatic_files)\n",
    "    \n",
    "    # Procesar imágenes (redimensionar y convertir a JPG)\n",
    "    processed_count = 0\n",
    "    for i, file_path in enumerate(tqdm(unique_files, desc=f\"Procesando {category}\")):\n",
    "        if processed_count >= max_images:\n",
    "            break\n",
    "        filename = f\"{category.replace(' ', '_')}_{i+1:04d}.jpg\"\n",
    "        if process_image(file_path, category_output_dir, filename):\n",
    "            processed_count += 1\n",
    "    \n",
    "    logging.info(f\"Procesadas {processed_count} imágenes para {category}\")\n",
    "    return processed_count"
   ],
   "id": "d1364df40c637908",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ejecución de la función main\n",
    "\n",
    "- La función main organiza el flujo del script, asegurando que las descargas y el procesamiento se realicen secuencialmente para cada categoría.\n",
    "- Crear los directorios al inicio evita errores si no existen.\n",
    "- Los mensajes de progreso mantienen al usuario informado sobre el estado del proceso.\n",
    "- El archivo de log proporciona un registro detallado para verificar el éxito del script o diagnosticar problemas."
   ],
   "id": "d9276a941af029a5"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-04-23T23:05:02.455839400Z",
     "start_time": "2025-04-22T07:01:31.051515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Crear directorios\n",
    "    Path(TEMP_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Descargar imágenes para cada categoría\n",
    "    for category in CATEGORIES:\n",
    "        count = download_images(category, TEMP_DIR, IMAGES_PER_CATEGORY)\n",
    "        print(f\"Descargadas {count} imágenes para {category}\")\n",
    "    \n",
    "    # Procesar imágenes para cada categoría\n",
    "    for category in CATEGORIES:\n",
    "        count = process_category(category, TEMP_DIR, OUTPUT_DIR, FINAL_IMAGES_PER_CATEGORY)\n",
    "        print(f\"Procesadas {count} imágenes para {category}\")\n",
    "        \n",
    "    print(f\"Procesamiento completado. Revisa el log en {LOG_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "a2a7097c317b602a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descargando imágenes de 'flowers' en: C:\\Users\\ctorr\\OneDrive\\Documentos\\Arturo\\Arturo HW ITESO\\6to semestre\\Modelos no lineales\\GANs\\data\\raw\\flowers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando flowers, página 1: 100%|██████████| 100/100 [00:49<00:00,  2.02it/s]\n",
      "Descargando flowers, página 2: 100%|██████████| 100/100 [00:50<00:00,  1.97it/s]\n",
      "Descargando flowers, página 3: 100%|██████████| 100/100 [00:50<00:00,  2.00it/s]\n",
      "Descargando flowers, página 4: 100%|██████████| 100/100 [00:51<00:00,  1.93it/s]\n",
      "Descargando flowers, página 5: 100%|██████████| 100/100 [00:49<00:00,  2.04it/s]\n",
      "Descargando flowers, página 6: 100%|██████████| 100/100 [00:51<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargadas 600 imágenes para flowers\n",
      "\n",
      "Descargando imágenes de 'mountains' en: C:\\Users\\ctorr\\OneDrive\\Documentos\\Arturo\\Arturo HW ITESO\\6to semestre\\Modelos no lineales\\GANs\\data\\raw\\mountains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando mountains, página 1: 100%|██████████| 100/100 [00:51<00:00,  1.94it/s]\n",
      "Descargando mountains, página 2: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "Descargando mountains, página 3: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n",
      "Descargando mountains, página 4: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Descargando mountains, página 5: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n",
      "Descargando mountains, página 6: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargadas 600 imágenes para mountains\n",
      "\n",
      "Descargando imágenes de 'animals' en: C:\\Users\\ctorr\\OneDrive\\Documentos\\Arturo\\Arturo HW ITESO\\6to semestre\\Modelos no lineales\\GANs\\data\\raw\\animals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando animals, página 1: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "Descargando animals, página 2: 100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n",
      "Descargando animals, página 3: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n",
      "Descargando animals, página 4: 100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n",
      "Descargando animals, página 5: 100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n",
      "Descargando animals, página 6: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargadas 600 imágenes para animals\n",
      "\n",
      "Descargando imágenes de 'everyday objects' en: C:\\Users\\ctorr\\OneDrive\\Documentos\\Arturo\\Arturo HW ITESO\\6to semestre\\Modelos no lineales\\GANs\\data\\raw\\everyday_objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando everyday objects, página 1: 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]\n",
      "Descargando everyday objects, página 2: 100%|██████████| 100/100 [01:16<00:00,  1.31it/s]\n",
      "Descargando everyday objects, página 3: 100%|██████████| 100/100 [01:20<00:00,  1.24it/s]\n",
      "Descargando everyday objects, página 4: 100%|██████████| 100/100 [01:16<00:00,  1.31it/s]\n",
      "Descargando everyday objects, página 5: 100%|██████████| 100/100 [01:20<00:00,  1.25it/s]\n",
      "Descargando everyday objects, página 6: 100%|██████████| 100/100 [01:19<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargadas 600 imágenes para everyday objects\n",
      "\n",
      "Descargando imágenes de 'urban environments' en: C:\\Users\\ctorr\\OneDrive\\Documentos\\Arturo\\Arturo HW ITESO\\6to semestre\\Modelos no lineales\\GANs\\data\\raw\\urban_environments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando urban environments, página 1: 100%|██████████| 100/100 [01:12<00:00,  1.39it/s]\n",
      "Descargando urban environments, página 2: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n",
      "Descargando urban environments, página 3: 100%|██████████| 100/100 [06:40<00:00,  4.01s/it]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aa2c53a4e0fcd949",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2a031757c3957133",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d8d4a8f7550a4e91",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
